{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rakuten_py_NLP_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKDabq7iY5aaUT/SCS2+Oz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ragdehl/Rakuten_py/blob/main/edgar/Rakuten_py_NLP_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAD6OuIKFzKO"
      },
      "source": [
        "# DEEP LEARNING TEXT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scMc7Je3GEQc"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySYaMINIFfu2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        " \n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1J6h3twGJ5e"
      },
      "source": [
        "Récuperer les données:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2DcZYNjAigc",
        "outputId": "c7e6401d-2539-4ea6-b8eb-e65d1565d490"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnl7nzsZAMvR"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voQEp-WCGN_J"
      },
      "source": [
        "# Initialiser la variable des mots vides\n",
        "stop_words = set(stopwords.words('french'))\n",
        " \n",
        "df_X = pd.read_csv('/content/drive/My Drive/Rakuten/X_train_update.csv',index_col=0)\n",
        "df_y = pd.read_csv('/content/drive/My Drive/Rakuten/Y_train_CVw08PX.csv',index_col=0)\n",
        " \n",
        "#df_X = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train_update.csv')\n",
        "#df_y = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\Y_train_CVw08PX.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU3ulrb2Agt6",
        "outputId": "1689d400-71e1-4a4f-a30f-a76ae47f60ca"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "def lemma(sentence): #Lemmatizer\n",
        "    doc = word_tokenize(sentence, language='french')\n",
        "    return [lemmatizer.lemmatize(token) for token in doc]\n",
        " \n",
        "def stop_words_filetring(mots) : \n",
        "    tokens = []\n",
        "    for mot in mots:\n",
        "        if mot not in stop_words:\n",
        "            tokens.append(mot)\n",
        "    return tokens\n",
        " \n",
        "def clean_text(text):\n",
        "    string = ''\n",
        "    words = word_tokenize(text.lower(), language='french')\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            #if word.isascii(): #and word.isalpha():\n",
        "            string += lemmatizer.lemmatize(word) + ' '\n",
        "    return string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        image_1263597046_product_3804725264.jpg\n",
              "1         image_1008141237_product_436067568.jpg\n",
              "2          image_938777978_product_201115110.jpg\n",
              "3           image_457047496_product_50418756.jpg\n",
              "4         image_1077757786_product_278535884.jpg\n",
              "                          ...                   \n",
              "84911      image_941495734_product_206719094.jpg\n",
              "84912    image_1188462883_product_3065095706.jpg\n",
              "84913     image_1009325617_product_440707564.jpg\n",
              "84914    image_1267353403_product_3942400296.jpg\n",
              "84915       image_684671297_product_57203227.jpg\n",
              "Name: image name, Length: 84916, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKC8kNTnAheK"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF0_tU-IAp3b"
      },
      "source": [
        "On netoye un peu le texte:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLj-_ra_Adiw"
      },
      "source": [
        "X = df_X.designation.astype(str) + ' ' + df_X.description.astype(str)\n",
        "y = df_y.prdtypecode\n",
        " \n",
        "X_clean = X.apply(lambda cell: clean_text(cell))\n",
        "X_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNF_asbzAyEO"
      },
      "source": [
        "Combien de mots uniques?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMJmfVL1Axfo"
      },
      "source": [
        "lis = []\n",
        "for element in X_clean.str.split():\n",
        "    for word in element:\n",
        "        lis.append(word)\n",
        "\n",
        "len(list(set(lis)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afwzH30A9zE"
      },
      "source": [
        "Vectoriser avec TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "vupRMm2eBBI8",
        "outputId": "90bc5d41-d6ff-440a-adee-c982a6dc4ce3"
      },
      "source": [
        "# Importer la classe train_test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Séparer le jeu de données en données d'entraînement et données test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1f3298604b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Séparer le jeu de données en données d'entraînement et données test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_clean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo9miGdRA4oP"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfid = TfidfVectorizer(analyzer='word',\n",
        "                  tokenizer=word_tokenize,\n",
        "                      #strip_accents='unicode',\n",
        "                      #stop_words=french_stop_words_no_accent, # peut etre interessant parce que lisse la progression\n",
        "                  max_df=0.8,\n",
        "                  min_df=2,\n",
        "                  ngram_range=(1,2),\n",
        "                  use_idf=True,\n",
        "                  smooth_idf=True,\n",
        "                  sublinear_tf=False,\n",
        "                  binary=True,\n",
        "                  )\n",
        "\n",
        "X_train_trans = tfid.fit_transform(X_train)\n",
        "X_test_trans = tfid.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nordwxChBpul"
      },
      "source": [
        "Deep Learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgDV0kRmBo8c"
      },
      "source": [
        "#vectorize the labels\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(y_train)\n",
        "one_hot_test_labels = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}