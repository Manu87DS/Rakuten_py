{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rakuten_py_NLP_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDR+h3C4/Jv6yHTu0h7Whw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ragdehl/Rakuten_py/blob/main/edgar/Rakuten_py_NLP_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAD6OuIKFzKO"
      },
      "source": [
        "# DEEP LEARNING TEXT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scMc7Je3GEQc"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySYaMINIFfu2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        " \n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1J6h3twGJ5e"
      },
      "source": [
        "Récuperer les données:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2DcZYNjAigc",
        "outputId": "f12a567a-d20f-41ba-926a-b3131567a136"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnl7nzsZAMvR",
        "outputId": "44064e75-bff4-4686-e81f-035de9bf808d"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voQEp-WCGN_J"
      },
      "source": [
        "# Initialiser la variable des mots vides\n",
        "stop_words = set(stopwords.words('french'))\n",
        " \n",
        "df_X = pd.read_csv('/content/drive/My Drive/Rakuten/X_train_update.csv',index_col=0)\n",
        "df_y = pd.read_csv('/content/drive/My Drive/Rakuten/Y_train_CVw08PX.csv',index_col=0)\n",
        " \n",
        "#df_X = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train_update.csv')\n",
        "#df_y = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\Y_train_CVw08PX.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU3ulrb2Agt6"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "def lemma(sentence): #Lemmatizer\n",
        "    doc = word_tokenize(sentence, language='french')\n",
        "    return [lemmatizer.lemmatize(token) for token in doc]\n",
        " \n",
        "def stop_words_filetring(mots) : \n",
        "    tokens = []\n",
        "    for mot in mots:\n",
        "        if mot not in stop_words:\n",
        "            tokens.append(mot)\n",
        "    return tokens\n",
        " \n",
        "def clean_text(text):\n",
        "    string = ''\n",
        "    words = word_tokenize(text.lower(), language='french')\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            #if word.isascii(): #and word.isalpha():\n",
        "            string += lemmatizer.lemmatize(word) + ' '\n",
        "    return string"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKC8kNTnAheK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddfed90-79ff-4d8d-efdd-12e865d005b9"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF0_tU-IAp3b"
      },
      "source": [
        "On netoye un peu le texte:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLj-_ra_Adiw",
        "outputId": "1bd0d410-41ba-462a-d0c3-fda2ad0ce16b"
      },
      "source": [
        "X = df_X.designation.astype(str) + ' ' + df_X.description.astype(str)\n",
        "y = df_y.prdtypecode\n",
        " \n",
        "X_clean = X.apply(lambda cell: clean_text(cell))\n",
        "X_clean"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        olivia : personalisiertes notizbuch / 150 seit...\n",
              "1        journal art ( ) n° 133 28/09/2001 - l'art marc...\n",
              "2        grand stylet ergonomique bleu gamepad nintendo...\n",
              "3        peluche donald - europe - disneyland 2000 ( ma...\n",
              "4        guerre tuques luc a id & eacute ; grandeur . v...\n",
              "                               ...                        \n",
              "84911                     the sims [ import anglais ] nan \n",
              "84912    kit piscine acier nevada déco pierre ø 3.50m x...\n",
              "84913    journal officiel republique francaise n° 46 15...\n",
              "84914    table basse bois récupération massif base blan...\n",
              "84915    gomme collection 2 gommes pinguin glace vert o...\n",
              "Length: 84916, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNF_asbzAyEO"
      },
      "source": [
        "Combien de mots uniques?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMJmfVL1Axfo",
        "outputId": "565fcb02-0bfd-4d4d-dbeb-cb2758e4a4f0"
      },
      "source": [
        "lis = []\n",
        "for element in X_clean.str.split():\n",
        "    for word in element:\n",
        "        lis.append(word)\n",
        "\n",
        "len(list(set(lis)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235642"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afwzH30A9zE"
      },
      "source": [
        "Vectoriser avec TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vupRMm2eBBI8"
      },
      "source": [
        "# Importer la classe train_test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Séparer le jeu de données en données d'entraînement et données test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y.astype(str),train_size = 0.1, test_size=0.02)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo9miGdRA4oP"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfid = TfidfVectorizer(analyzer='word',\n",
        "                  tokenizer=word_tokenize,\n",
        "                      #strip_accents='unicode',\n",
        "                      #stop_words=french_stop_words_no_accent, # peut etre interessant parce que lisse la progression\n",
        "                  max_df=0.8,\n",
        "                  min_df=2,\n",
        "                  ngram_range=(1,2),\n",
        "                  use_idf=True,\n",
        "                  smooth_idf=True,\n",
        "                  sublinear_tf=False,\n",
        "                  binary=True,\n",
        "                  )\n",
        "\n",
        "X_train_trans = tfid.fit_transform(X_train)\n",
        "X_test_trans = tfid.transform(X_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTt6WTmLH-9r"
      },
      "source": [
        "X_train_trans = X_train_trans.todense()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxVl0O-8IDT4"
      },
      "source": [
        "X_test_trans = X_test_trans.todense()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjcLY6cGYuAL",
        "outputId": "8cb9b04b-285c-44ef-d7e3-5d36551b5177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_dim = X_train_trans.shape[1]\n",
        "input_dim"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nordwxChBpul"
      },
      "source": [
        "Deep Learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgDV0kRmBo8c"
      },
      "source": [
        "#vectorize the labels\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(y_train)\n",
        "one_hot_test_labels = to_categorical(y_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I9L_4KMC17p",
        "outputId": "6dac1aea-3193-4b9c-c24f-a6624c800898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_dim=input_dim))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 64)                5311168   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 46)                2990      \n",
            "=================================================================\n",
            "Total params: 5,318,318\n",
            "Trainable params: 5,318,318\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg9JYbA0DbgB"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DuSuRrKERyY"
      },
      "source": [
        "history = model.fit(X_train_trans,\n",
        "                    one_hot_train_labels,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_test_trans, one_hot_test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}