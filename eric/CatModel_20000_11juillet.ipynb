{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2ccdba",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Infos d'install\n",
    "\n",
    "  * conda install python=3.7.10  \n",
    "  * conda install spacy=2.3.5  \n",
    "  * python -m spacy download fr_core_news_sm   \n",
    "  * conda install tensorflow-hub  \n",
    "  * pip install --user tensorflow-text  \n",
    "      *# WARNING: The script tensorboard.exe is installed in .. which is not on PATH*\n",
    "   * télécharger https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3 dans tfhub\n",
    "\n",
    "\n",
    "## Liens    \n",
    "  * Questions keras sur les générateurs https://github.com/keras-team/keras/issues/3386\n",
    "  * Custom generator https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3\n",
    "  * http://artelab.dista.uninsubria.it/res/research/papers/2020/2020-IVCNZ-Gallo-Food101.pdf  \n",
    "  * http://cbonnett.github.io/Insight.html  \n",
    "  * https://stackoverflow.com/questions/51696575/keras-cnn-add-text-as-additional-input-besides-image-to-cnn  \n",
    "  * https://github.com/depshad/Deep-Learning-Framework-for-Multi-modal-Product-Classification/blob/master/camembert_train_predict.ipynb  \n",
    "  * https://keras.io/examples/nlp/pretrained_word_embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7d143ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import cv2 # opencv\n",
    "import datetime\n",
    "import hashlib\n",
    "import inspect\n",
    "import pickle\n",
    "import wget\n",
    "import gzip\n",
    "import time\n",
    "import tqdm\n",
    "import html\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import tensorflow_text # Needed for sentencepiece\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D,\\\n",
    "                                    Input, Embedding, GRU, Bidirectional, \\\n",
    "                                    Conv1D, MaxPooling1D, GlobalMaxPooling1D, \\\n",
    "                                    BatchNormalization, concatenate\n",
    "NB_ECHANTILLONS = 20000\n",
    "\n",
    "# Données d'entrée (lecture seule)\n",
    "X_TRAIN_CSV_FILE = \"X_train_update.csv\"\n",
    "Y_TRAIN_CSV_FILE = \"Y_train_CVw08PX.csv\"\n",
    "X_TEST_CSV_FILE = \"X_test_update.csv\"\n",
    "NB_CLASSES = 27\n",
    "\n",
    "# Données de travail et de sortie\n",
    "OUTDIR = \"modele_rakuten_out\"\n",
    "\n",
    "MULTILINGUAL_DIR = \"tfhub/universal-sentence-encoder-multilingual-large-3\"\n",
    "MULTILINGUAL_LINK = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"\n",
    "\n",
    "def init_OUTDIR():\n",
    "    \"\"\" \n",
    "    Initialise le répertoire où seront générés les fichiers.\n",
    "    Les fichiers d'entrées y sont copiés après mélange, au cas où ils\n",
    "    auraient un ordre particulier\n",
    "    \"\"\"\n",
    "    xtrainfile = os.path.join(OUTDIR, os.path.basename(X_TRAIN_CSV_FILE))\n",
    "    ytrainfile = os.path.join(OUTDIR, os.path.basename(Y_TRAIN_CSV_FILE))\n",
    "    xtestfile = os.path.join(OUTDIR, os.path.basename(X_TEST_CSV_FILE))\n",
    "    if os.path.isfile(xtrainfile) and os.path.isfile(ytrainfile) and \\\n",
    "       os.path.isfile(xtestfile):\n",
    "        return\n",
    "    print('Création des fichiers entrées après mélange (shuffle)')\n",
    "    if not os.path.isdir(OUTDIR):\n",
    "        os.makedirs(OUTDIR)\n",
    "        if not os.path.isdir(OUTDIR):\n",
    "            raise ValueError(f\"Impossible de créer {OUTDIR}\")\n",
    "    dfx = pd.read_csv(X_TRAIN_CSV_FILE)\n",
    "    dfy = pd.read_csv(Y_TRAIN_CSV_FILE)\n",
    "    dfx, dfy = shuffle(dfx, dfy, random_state=51)\n",
    "    dfx, dfy = shuffle(dfx, dfy, random_state=52)\n",
    "    dfx.to_csv(xtrainfile)\n",
    "    dfy.to_csv(ytrainfile)\n",
    "    dfx = pd.read_csv(X_TEST_CSV_FILE)\n",
    "    dfx = shuffle(dfx, random_state=53)\n",
    "    dfx = shuffle(dfx, random_state=54)\n",
    "    dfx.to_csv(xtestfile)\n",
    "\n",
    "def image_path(row, subdir):\n",
    "        f = \"image_%d_product_%d.jpg\" % (row.imageid, row.productid) \n",
    "        ff = os.path.join(os.getcwd(), \"images\", subdir, f)\n",
    "        return ff if os.path.isfile(ff) else None\n",
    "    \n",
    "def get_y():\n",
    "    \"\"\" Retourne les y (cibles) \"\"\"\n",
    "    init_OUTDIR()\n",
    "    ytrainfile = os.path.join(OUTDIR, os.path.basename(Y_TRAIN_CSV_FILE))\n",
    "    return pd.read_csv(ytrainfile)[\"prdtypecode\"].astype(str)\n",
    "\n",
    "    \n",
    "def get_X_text(inputXfile=None):\n",
    "    \"\"\"\n",
    "    Retourne une liste X de phrases pré-traitées à partir d'un dataframe\n",
    "    inputXfile qui contient les colonnes \"designation\" et \"description\"\n",
    "    (cette dernière possèdent des valeurs NA)\n",
    "    \"\"\"\n",
    "    if inputXfile is None:\n",
    "        inputXfile = os.path.join(OUTDIR, os.path.basename(X_TRAIN_CSV_FILE))\n",
    "    f = re.sub(r\"\\.csv$\", \"_text.pkl\", os.path.basename(inputXfile))\n",
    "    xtextfile = os.path.join(OUTDIR, f)\n",
    "    if os.path.isfile(xtextfile):\n",
    "        with open(xtextfile, 'rb') as fd:\n",
    "            return pickle.load(fd)\n",
    "    print(f\"Creation de {xtextfile}\")\n",
    "    init_OUTDIR()\n",
    "    X = []\n",
    "    df = pd.read_csv(inputXfile)\n",
    "    for desi, desc in zip(df.designation, df.description):\n",
    "        desistr = desi if type(desi) == str else ''\n",
    "        descstr = desc if type(desc) == str else ''\n",
    "        s = (desistr + '. DESCRIPTION: ' + descstr) if len(descstr) > 0 else desistr\n",
    "        X.append(s)\n",
    "    d = os.path.dirname(xtextfile)\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "    pickle.dump(X, open(xtextfile, 'wb'))\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_X_image_path(inputXfile=None):\n",
    "    \"\"\"\n",
    "    Retourne une liste X de chemins des fichiers images à partir d'un\n",
    "    dataframe inputXfile\n",
    "    \"\"\"\n",
    "    if inputXfile is None:\n",
    "        inputXfile = os.path.join(OUTDIR, os.path.basename(X_TRAIN_CSV_FILE))\n",
    "    f = re.sub(r\"\\.csv$\", \"_image_path.pkl\", os.path.basename(inputXfile))\n",
    "    xpathfile = os.path.join(OUTDIR, f)\n",
    "    if os.path.isfile(xpathfile):\n",
    "        with open(xpathfile, 'rb') as fd:\n",
    "            return pickle.load(fd)\n",
    "    print(f\"Creation de {xpathfile}\")\n",
    "    init_OUTDIR()\n",
    "    df = pd.read_csv(inputXfile)\n",
    "    subdir = \"image_train\" if 'train' in inputXfile else \"image_test\"\n",
    "    X = list(df.apply(lambda x: image_path(x, subdir), axis=1))\n",
    "    d = os.path.dirname(xpathfile)\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "    pickle.dump(X, open(xpathfile, 'wb'))\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_X_text_spacy_lemma(inputXfile=None):\n",
    "    \"\"\"\n",
    "    Retourne une liste X de tokens créés par le tokenizer.lemma de Spacy\n",
    "    \"\"\"\n",
    "    if inputXfile is None:\n",
    "        inputXfile = os.path.join(OUTDIR, os.path.basename(X_TRAIN_CSV_FILE))\n",
    "    f = re.sub(r\"\\.csv$\", \"_text_spacy_lemma.pkl\",\n",
    "               os.path.basename(inputXfile))\n",
    "    xtokenfile = os.path.join(OUTDIR, f)\n",
    "    if os.path.isfile(xtokenfile):\n",
    "        with open(xtokenfile, 'rb') as fd:\n",
    "            return pickle.load(fd)\n",
    "    X_text = get_X_text(inputXfile)\n",
    "    print(f\"Tokenization de {len(X_text)} phrases\")\n",
    "    spacynlp = spacy.load(\"fr_core_news_sm\")\n",
    "    spacynlp.disable_pipes('tagger', 'parser', 'ner')\n",
    "    X = []\n",
    "    for sentence in tqdm.tqdm(X_text):\n",
    "        sentence = re.sub(r\"([?¿.!,:;])\", r\" \\1 \", sentence) # Isole la ponctuation\n",
    "        tokens = [x.lemma_ for x in spacynlp(sentence)]\n",
    "        X.append(tokens)\n",
    "    #X = np.array(X).reshape(len(X_text),-1)\n",
    "    d = os.path.dirname(xtokenfile)\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "    pickle.dump(X, open(xtokenfile, 'wb'))\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_X_text_spacy_lemma_lower(inputXfile=None):\n",
    "    \"\"\"\n",
    "    Retourne une liste X de tokens créés par le tokenizer.lemma de Spacy\n",
    "    et transformation en caractètes minuscules\n",
    "    \"\"\"\n",
    "    if inputXfile is None:\n",
    "        inputXfile = os.path.join(OUTDIR, os.path.basename(X_TRAIN_CSV_FILE))\n",
    "    f = re.sub(r\"\\.csv$\", \"_text_spacy_lemma_lower.pkl\",\n",
    "               os.path.basename(inputXfile))\n",
    "    xtokenfile = os.path.join(OUTDIR, f)\n",
    "    if os.path.isfile(xtokenfile):\n",
    "        with open(xtokenfile, 'rb') as fd:\n",
    "            return pickle.load(fd)\n",
    "    X_raw = get_X_text_spacy_lemma(inputXfile)\n",
    "    print(f\"Mise en minuscule de {len(X_raw)} listes de tokens\")\n",
    "    X = []\n",
    "    for sentence in tqdm.tqdm(X_raw):\n",
    "        tokens = [x.lower() for x in sentence]\n",
    "        X.append(tokens)\n",
    "    d = os.path.dirname(xtokenfile)\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "    pickle.dump(X, open(xtokenfile, 'wb'))\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_X_text_embed_multilingual(inputXfile=None):\n",
    "    \"\"\"\n",
    "    Retourne une liste X de vecteurs de plongement dans le modèle pre-entrainé\n",
    "    \"multilingual-large-3\"\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(MULTILINGUAL_DIR):\n",
    "        raise ValueError(f\"Répertoire non trouvé {MULTILINGUAL_DIR}\")\n",
    "        \n",
    "    if inputXfile is None:\n",
    "        inputXfile = os.path.join(OUTDIR, os.path.basename(X_TRAIN_CSV_FILE))\n",
    "    f = re.sub(r\"\\.csv$\", \"_text_embed_multilingual.pkl\",\n",
    "               os.path.basename(inputXfile))\n",
    "    xembedfile = os.path.join(OUTDIR, f)\n",
    "    if os.path.isfile(xembedfile):\n",
    "        with open(xembedfile, 'rb') as fd:\n",
    "            return pickle.load(fd)\n",
    "    X_text = get_X_text(inputXfile)\n",
    "    print(f\"Chargement de {MULTILINGUAL_DIR}\")\n",
    "    embed = hub.load(MULTILINGUAL_DIR)\n",
    "    print(f\"Plongement de {len(X_text)} phrases\")\n",
    "    X = []\n",
    "    for x in tqdm.tqdm(X_text):\n",
    "        X.append(embed(x)) # phrase x traduite en vecteur embedding\n",
    "    X = np.array(X).reshape(len(X_text),-1)\n",
    "    d = os.path.dirname(xembedfile)\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "    pickle.dump(X, open(xembedfile, 'wb'))\n",
    "    return X\n",
    "\n",
    "\n",
    "def plot_history(title, history):\n",
    "    \"\"\"\n",
    "    Affiche les évolution par epoque de la perte et de l'accuracy\n",
    "    \"\"\"\n",
    "    if len(history.history['loss']) <= 1:\n",
    "        return\n",
    "    plt.figure(figsize=(15,3))\n",
    "    for i, s in enumerate(['loss', 'accuracy']):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.plot(history.history[s], 'bo', color='orange')\n",
    "        plt.plot(history.history['val_' + s], 'b', color='blue')\n",
    "        plt.title(f'{title}: {s} by epoch')\n",
    "        plt.ylabel(s)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='right')\n",
    "    plt.show()\n",
    "\n",
    "class RakutenBaseModel:\n",
    "    \"\"\"\n",
    "    Classe de base héritée par les modèles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, fit_length=None):\n",
    "            \n",
    "        self.outdir = OUTDIR\n",
    "        self.name = name\n",
    "        self.batch_size = 32\n",
    "        self.validation_split = 0.15\n",
    "        self.fbestweights = os.path.join(self.outdir,\n",
    "                                         self.name + '_bestweights.hdf5')\n",
    "        if fit_length is not None:\n",
    "            fprefix = os.path.join(self.outdir, f\"{self.name}_{fit_length}\")\n",
    "            objectfile = fprefix + \"_object.pkl\"\n",
    "            modelfile = fprefix + \"_model.hdf5\"\n",
    "            if not os.path.isfile(objectfile):\n",
    "                raise ValueError(f\"Pas de fichier {objectfile}\")\n",
    "            print(f\"Chargement de l'objet ({objectfile})\")\n",
    "            obj = pickle.load(open(objectfile, \"rb\"))\n",
    "            objvars = vars(obj)\n",
    "            for v in objvars:\n",
    "                setattr(self, v, objvars[v])\n",
    "            if os.path.isfile(modelfile):\n",
    "                print(f\"Chargement du modèle ({modelfile})\")\n",
    "                self.model = tf.keras.models.load_model(modelfile)\n",
    "        return self\n",
    "\n",
    "    def prt(self, msg):\n",
    "        \"\"\" Imprime un message et met à jour la variable 'journal' \"\"\"\n",
    "        if not hasattr(self, 'journal'):\n",
    "            self.journal = ''\n",
    "        now = datetime.datetime.now().strftime(\"%Hh%Mmn\")\n",
    "        s = f\"++ [{now}] {self.name}: {msg}\"\n",
    "        self.journal += s + '\\n'\n",
    "        print(s)\n",
    "\n",
    "    def report(self, y_test=None, y_pred=None):\n",
    "        \"\"\" Affiche un rapport \"\"\"\n",
    "        score = round(f1_score(y_test, y_pred, average='weighted'), 4)\n",
    "        self.prt(f'w-f1-score = \\033[1m{score}\\033[0m\\n')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        return score\n",
    "\n",
    "    def get_model(self, off_start, off_end, input_file=None):\n",
    "        inp, x = self.get_model_body(off_start, off_end, input_file)\n",
    "        x = tf.keras.layers.Dense(NB_CLASSES, activation='softmax', name='dense_' + self.name)(x)\n",
    "        return Model(inp, x)\n",
    "\n",
    "    def preprocess_y_train(self, off_start, off_end, input_file=None):\n",
    "        y_train = get_y()[off_start:off_end]\n",
    "        self.fit_labels = {i: v for i, v in enumerate(sorted(list(set(y_train))))}\n",
    "        assert len(self.fit_labels) == NB_CLASSES\n",
    "        rv = {self.fit_labels[i]: i for i in self.fit_labels}\n",
    "        y_train = np.array([rv[v] for v in y_train])\n",
    "        return y_train\n",
    "\n",
    "    def layer_name(self, s):\n",
    "        self.layer_index += 1\n",
    "        return f\"{s}_{self.layer_index}_{self.name}\"\n",
    "\n",
    "    def compile_and_train_gen(self, X_train, y_train,\n",
    "                              optimizer='adam',\n",
    "                              epochs=10,\n",
    "                              patience_stop=2, patience_lr=None,\n",
    "                              class_weight=[],\n",
    "                              callbacks=[]):\n",
    "        self.prt(\"fit(): Début\")\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience_stop,\n",
    "                                             restore_best_weights=True, verbose=1),\n",
    "                     tf.keras.callbacks.ModelCheckpoint(filepath=self.fbestweights,\n",
    "                                             save_weights_only=True, save_best_only=True,\n",
    "                                              monitor='val_loss', mode='min')]\n",
    "        if patience_lr is not None:\n",
    "            callbacks += [tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                                monitor='val_loss', factor=0.1, patience=patience_lr, verbose=1)]\n",
    "\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            os.remove(self.fbestweights)\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics = ['accuracy'])\n",
    "        history = self.model.fit(X_train, y_train,\n",
    "                                 epochs = epochs,\n",
    "                                 validation_split = self.validation_split,\n",
    "#                                steps_per_epoch = traingen.n//traingen.batch_size,\n",
    "#                                validation_data = valgen,\n",
    "#                                validation_steps = valgen.n//valgen.batch_size,\n",
    "                                callbacks = callbacks, class_weight = class_weight)\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            self.model.load_weights(self.fbestweights)\n",
    "        plot_history(f\"{self.name}\", history)\n",
    "        self.prt(\"fit(): Fin\\n\")\n",
    "        return history\n",
    "        \n",
    "    def predict(self, off_start, off_end, input_file=None):\n",
    "        X_test = self.preprocess_X_test(off_start, off_end, input_file)\n",
    "         \n",
    "        self.prt(\"predict(): Début\")\n",
    "        softmaxout = self.model.predict(X_test, verbose = 1)\n",
    "        y_pred = [self.fit_labels[i] for i in np.argmax(softmaxout, axis=1)]\n",
    "        self.prt(\"predict(): Fin\\n\")\n",
    "        return y_pred\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\" Sauvegarde sur disque \"\"\"\n",
    "        if not hasattr(self, 'fit_length') or self.fit_length == 0:\n",
    "            self.prt(\"Il n'y a rien à sauvegarder, fit n'a pas été exécuté)\")\n",
    "            return None\n",
    "        fprefix = os.path.join(self.outdir,\n",
    "                         f\"{self.name}_{self.fit_length}\")\n",
    "        if hasattr(self, \"model\") and hasattr(self.model, \"save\"):\n",
    "            f = fprefix + \"_model.hdf5\"\n",
    "            self.model.save(f)\n",
    "            self.prt(f\"Modèle sauvegardé dans {f}\")\n",
    "            self.model = None\n",
    "        f = fprefix + \"_object.pkl\"\n",
    "        tmpf = f + '_tmp'\n",
    "        selfvars = vars(self)\n",
    "        weaks = []\n",
    "        for v in selfvars:\n",
    "            try:\n",
    "                pickle.dump(selfvars[v], open(tmpf, 'wb'))\n",
    "            except:\n",
    "                print(\"Pas de sauvegarde de \", v)\n",
    "                weaks.append(v)\n",
    "        for v in weaks:\n",
    "            del(selfvars[v])\n",
    "        if os.path.isfile(tmpf):\n",
    "            os.remove(tmpf)\n",
    "        pickle.dump(self, open(f, 'wb'))\n",
    "        self.prt(f\"Objet complet sauvegardé dans {f}\")\n",
    "        return f\n",
    "    \n",
    "    def evaluate(self, samples_number=-1, test_size=0.2, train_offset=0):\n",
    "        \"\"\" Cycle complet fit + predict + report + save \"\"\"\n",
    "        if samples_number == -1: # On prend tous le  jeu d'entrainement\n",
    "            samples_number = len(get_y()) - train_offset\n",
    "        t0 = time.time()\n",
    "        test_offset = int(samples_number * (1 - test_size))\n",
    "        self.fit(train_offset, test_offset)\n",
    "        y_test = get_y()[test_offset : train_offset + samples_number]\n",
    "        y_pred = self.predict(test_offset, train_offset + samples_number)\n",
    "        self.report(y_test, y_pred)\n",
    "        t = int(time.time() - t0)\n",
    "        self.prt(f\"Evaluation exécutée en {t} secondes\")\n",
    "        self.save()\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\" Création du modèle \"\"\"\n",
    "        t0 = time.time()\n",
    "        nb = len(get_y())\n",
    "        self.fit(0, nb)\n",
    "        self.save()\n",
    "        df = pd.read_csv(X_TEST_CSV_FILE)\n",
    "        # ... faire un shuffle avant predict, remettre dans l'ordre ensuite\n",
    "        y_pred = self.predict(0, df.shape[0], input_file=X_TEST_CSV_FILE)\n",
    "        df['prdtypecode'] = y_pred\n",
    "        csvfile = os.path.join(self.outdir, f\"test_update_{self.name}.csv\")\n",
    "        df.to_csv(csvfile)\n",
    "        t = int(time.time() - t0)\n",
    "        self.prt(f\"{csvfile} crée en {t} secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbe5f99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ [14h31mn] TextEmbed: Entrainement de 16000 échantillons\n",
      "++ [14h31mn] TextEmbed: Nombre de mots max par phrase originel = 2551\n",
      "++ [14h31mn] TextEmbed: Nombre de mots max par phrase utilisé  = 600\n",
      "++ [14h31mn] TextEmbed: Taille du vocabulaire (nombre de mots) = 73792\n",
      "++ [14h31mn] TextEmbed: fit(): Début\n",
      "Epoch 1/50\n",
      "425/425 [==============================] - 95s 223ms/step - loss: 2.9104 - accuracy: 0.1772 - val_loss: 2.4852 - val_accuracy: 0.4375\n",
      "Epoch 2/50\n",
      "425/425 [==============================] - 91s 215ms/step - loss: 1.6938 - accuracy: 0.5256 - val_loss: 1.4046 - val_accuracy: 0.6158\n",
      "Epoch 3/50\n",
      "425/425 [==============================] - 90s 212ms/step - loss: 1.0449 - accuracy: 0.7160 - val_loss: 1.3011 - val_accuracy: 0.6296\n",
      "Epoch 4/50\n",
      "425/425 [==============================] - 90s 213ms/step - loss: 0.6737 - accuracy: 0.8138 - val_loss: 1.1211 - val_accuracy: 0.6833\n",
      "Epoch 5/50\n",
      "425/425 [==============================] - 90s 212ms/step - loss: 0.4945 - accuracy: 0.8671 - val_loss: 0.9957 - val_accuracy: 0.7337\n",
      "Epoch 6/50\n",
      "425/425 [==============================] - 90s 212ms/step - loss: 0.3350 - accuracy: 0.9121 - val_loss: 1.0563 - val_accuracy: 0.7325\n",
      "Epoch 7/50\n",
      "425/425 [==============================] - 90s 213ms/step - loss: 0.2658 - accuracy: 0.9274 - val_loss: 1.1740 - val_accuracy: 0.7204\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/50\n",
      "425/425 [==============================] - 89s 209ms/step - loss: 0.1579 - accuracy: 0.9618 - val_loss: 0.9378 - val_accuracy: 0.7729\n",
      "Epoch 9/50\n",
      "425/425 [==============================] - 88s 207ms/step - loss: 0.1419 - accuracy: 0.9650 - val_loss: 0.9491 - val_accuracy: 0.7683\n",
      "Epoch 10/50\n",
      "425/425 [==============================] - 88s 207ms/step - loss: 0.1292 - accuracy: 0.9684 - val_loss: 0.9268 - val_accuracy: 0.7750\n",
      "Epoch 11/50\n",
      "425/425 [==============================] - 88s 207ms/step - loss: 0.1259 - accuracy: 0.9703 - val_loss: 0.9900 - val_accuracy: 0.7600\n",
      "Epoch 12/50\n",
      "425/425 [==============================] - 88s 207ms/step - loss: 0.1162 - accuracy: 0.9710 - val_loss: 0.9437 - val_accuracy: 0.7779\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/50\n",
      "425/425 [==============================] - 88s 207ms/step - loss: 0.1151 - accuracy: 0.9737 - val_loss: 0.9447 - val_accuracy: 0.7758\n",
      "Epoch 14/50\n",
      "425/425 [==============================] - 88s 207ms/step - loss: 0.1111 - accuracy: 0.9742 - val_loss: 0.9448 - val_accuracy: 0.7767\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ [14h52mn] TextEmbed: fit(): Fin\n",
      "\n",
      "++ [14h52mn] TextEmbed: predict(): Début\n",
      "4000/4000 [==============================] - ETA:  - 3s 660us/step\n",
      "++ [14h53mn] TextEmbed: predict(): Fin\n",
      "\n",
      "++ [14h53mn] TextEmbed: w-f1-score = \u001b[1m0.7654\u001b[0m\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.58      0.59      0.59       155\n",
      "        1140       0.82      0.70      0.75       128\n",
      "        1160       0.87      0.90      0.89       168\n",
      "        1180       0.38      0.38      0.38        32\n",
      "        1280       0.55      0.53      0.54       214\n",
      "        1281       0.54      0.52      0.53        85\n",
      "        1300       0.80      0.79      0.80       228\n",
      "        1301       0.97      0.84      0.90        44\n",
      "        1302       0.85      0.73      0.79       132\n",
      "        1320       0.61      0.73      0.66       136\n",
      "        1560       0.69      0.80      0.74       242\n",
      "        1920       0.91      0.85      0.88       222\n",
      "        1940       0.82      0.77      0.79        35\n",
      "        2060       0.74      0.68      0.71       248\n",
      "        2220       0.95      0.73      0.83        49\n",
      "        2280       0.75      0.88      0.81       233\n",
      "        2403       0.76      0.81      0.79       256\n",
      "        2462       0.68      0.67      0.68        67\n",
      "        2522       0.85      0.92      0.88       216\n",
      "        2582       0.70      0.60      0.65       123\n",
      "        2583       0.96      0.95      0.96       466\n",
      "        2585       0.68      0.70      0.69       114\n",
      "        2705       0.66      0.78      0.72       136\n",
      "        2905       0.96      0.96      0.96        49\n",
      "          40       0.72      0.49      0.58       113\n",
      "          50       0.71      0.66      0.68        70\n",
      "          60       0.86      0.82      0.84        39\n",
      "\n",
      "    accuracy                           0.77      4000\n",
      "   macro avg       0.75      0.73      0.74      4000\n",
      "weighted avg       0.77      0.77      0.77      4000\n",
      "\n",
      "++ [14h53mn] TextEmbed: Evaluation exécutée en 1265 secondes\n",
      "++ [14h53mn] TextEmbed: Modèle sauvegardé dans modele_rakuten_out\\TextEmbed_16000_model.hdf5\n",
      "++ [14h53mn] TextEmbed: Objet complet sauvegardé dans modele_rakuten_out\\TextEmbed_16000_object.pkl\n"
     ]
    }
   ],
   "source": [
    "class TextEmbed(RakutenBaseModel):\n",
    "    \"\"\"\n",
    "    Modèle avec une première couche Embedding()\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_length=None):\n",
    "        super().__init__(self.__class__.__name__, fit_length)\n",
    "        self.maxsentwords = 600 # Nombre max de mots par phrase (tronquées)\n",
    "        self.embeddim = 200   # Taille des vecteurs d'embedding\n",
    "                              # 300 => 0.7996 pour 20000 maias ca depend des fois\n",
    "                              # 200 => 0.7621      \"\"\n",
    "\n",
    "    def get_model_body(self, off_start, off_end, input_file=None):\n",
    "        self.layer_index = 0\n",
    "        inp = tf.keras.layers.Input(shape=(self.maxsentwords,), name=self.layer_name(\"input\"))\n",
    "        x = tf.keras.layers.Embedding(input_dim=self.vocablen, output_dim=self.embeddim,\n",
    "                                      input_length=self.maxsentwords,\n",
    "                                      name=self.layer_name(\"embedding\"))(inp)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D(name=self.layer_name(\"GAvePool1D\"))(x)\n",
    "        x = tf.keras.layers.Dense(128, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.Dropout(0.4, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=self.layer_name(\"batchnorm\"))(x)\n",
    "        return inp, x\n",
    "\n",
    "    def text_to_nparray(self, X):\n",
    "        self.word2num = {w: i+1 for i, w in enumerate(self.vocab)}\n",
    "        self.num2word = {self.word2num[w]: w for w in self.word2num}\n",
    "        X_new = np.zeros((len(X), self.maxsentwords), dtype=int)\n",
    "        for i, sentence in enumerate(X):\n",
    "            for j, w in enumerate(sentence[:self.maxsentwords]):\n",
    "                if w in self.word2num:\n",
    "                    X_new[i,j] = self.word2num[w]\n",
    "        return X_new\n",
    "    \n",
    "    def preprocess_X_train(self, off_start, off_end, input_file=None):\n",
    "        self.prt(f\"Entrainement de {self.fit_length} échantillons\")\n",
    "        X_train = get_X_text_spacy_lemma_lower(input_file)[off_start:off_end]\n",
    " \n",
    "        maxnb = max([len(sent) for sent in X_train])\n",
    "        self.prt(f\"Nombre de mots max par phrase originel = {maxnb}\")\n",
    "        self.prt(f\"Nombre de mots max par phrase utilisé  = {self.maxsentwords}\")\n",
    "        self.vocab = sorted(list({x for sentence in X_train \\\n",
    "                                                for x in sentence[:self.maxsentwords]}))\n",
    "        self.vocablen = len(self.vocab) + 1 # +1 pour l'id 0 qui correspond au remplissage\n",
    "        self.prt(f\"Taille du vocabulaire (nombre de mots) = {self.vocablen}\")\n",
    "        X_train = self.text_to_nparray(X_train)\n",
    "        return X_train\n",
    "\n",
    "    def preprocess_X_test(self, off_start, off_end, input_file=None):\n",
    "        X_test = get_X_text_spacy_lemma_lower(input_file)[off_start:off_end]\n",
    "        X_test = self.text_to_nparray(X_test)\n",
    "        return X_test\n",
    "\n",
    "    def fit(self, off_start, off_end, input_file=None):\n",
    "        self.fit_length = off_end - off_start\n",
    "        X_train = self.preprocess_X_train(off_start, off_end, input_file)\n",
    "        y_train = self.preprocess_y_train(off_start, off_end, input_file)\n",
    "        \n",
    "        self.model = self.get_model(off_start, off_end, input_file)\n",
    "         \n",
    "        return self.compile_and_train_gen(X_train, y_train, optimizer='adam',\n",
    "                                         epochs=50, patience_stop=4, patience_lr=2,\n",
    "                                         class_weight=[])\n",
    "\n",
    "\n",
    "TextEmbed().evaluate(NB_ECHANTILLONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f03503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextOneHot(RakutenBaseModel):\n",
    "    \"\"\"\n",
    "    Modèle NN exécuté après un préprocessing des données effectué\n",
    "    avec un pipe sklearn <TfidfVectorizer | SelectFromModel(LinearSVC)>\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_length=None):\n",
    "        super().__init__(self.__class__.__name__, fit_length)\n",
    "        self.maxfeatures = 25000 # Nombre de features selectionnées par SVC\n",
    "                \n",
    "        self.select = make_pipeline(\n",
    "            TfidfVectorizer(analyzer='word',\n",
    "                  #strip_accents='ascii',\n",
    "                  #stop_words=french_stop_words, \n",
    "                  # tokenizer=tokenize_spacy, mieux mais long\n",
    "                  #lowercase=True,\n",
    "                  max_df=0.8,\n",
    "                  min_df=2,\n",
    "                  ngram_range=(1,2),\n",
    "                  use_idf=True,\n",
    "                  smooth_idf=True,\n",
    "                  sublinear_tf=False,\n",
    "                  binary=True,\n",
    " #                 max_features=30000,  moins rentable que celui de SelectFromModel \n",
    "                  ),\n",
    "                  SelectFromModel(LinearSVC(penalty=\"l2\", dual=True, C=0.8,\n",
    "                                            tol=1e-5, max_iter=4000),\n",
    "                                  max_features=self.maxfeatures))\n",
    "\n",
    "    def get_model_body(self, off_start, off_end, input_file=None):\n",
    "        self.layer_index = 0\n",
    "        X_train = self.preprocess_X_train(off_start, off_end, input_file)\n",
    "        inp = tf.keras.layers.Input(shape=X_train.shape[1:], name=self.layer_name(\"input\"))\n",
    "        x = tf.keras.layers.Dense(100, activation='relu', name=self.layer_name(\"dense\"))(inp)\n",
    "        x = tf.keras.layers.Dropout(0.5, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.Dropout(0.4, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=self.layer_name(\"batchnorm\"))(x)\n",
    "        return inp, x\n",
    "\n",
    "    def preprocess_X_train(self, off_start, off_end, input_file=None):\n",
    "        y_train_select = get_y()[off_start:off_end]\n",
    "        X_train = get_X_text(input_file)[off_start:off_end]\n",
    "        X_train = self.select.fit_transform(X_train, y_train_select).toarray()\n",
    "        return X_train\n",
    "\n",
    "    def preprocess_X_test(self, off_start, off_end, input_file=None):\n",
    "        X_test = get_X_text(input_file)[off_start:off_end]\n",
    "        X_test = self.select.transform(X_test).toarray()\n",
    "        return X_test\n",
    "\n",
    "    def fit(self, off_start, off_end, input_file=None):\n",
    "        self.fit_length = off_end - off_start\n",
    "        self.model = self.get_model(off_start, off_end, input_file)\n",
    "\n",
    "        X_train = self.preprocess_X_train(off_start, off_end, input_file)\n",
    "        y_train = self.preprocess_y_train(off_start, off_end, input_file)\n",
    "\n",
    "        return self.compile_and_train_gen(X_train, y_train, optimizer='adam',\n",
    "                                         epochs=50, patience_stop=5, patience_lr=3,\n",
    "                                         class_weight=[])\n",
    "        \n",
    "TextOneHot().evaluate(NB_ECHANTILLONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe6881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilingu(RakutenBaseModel):\n",
    "    \"\"\"\n",
    "    Modèle de plongement (embedding) dans un modèle de texte préentrainé\n",
    "    (Multilingual-large)\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_length=None):\n",
    "        super().__init__(self.__class__.__name__, fit_length)\n",
    "        self.embedding_length = 512 # Défini par le modèle Multilingual/large\n",
    "        self.std = StandardScaler()\n",
    "\n",
    "    def get_model_body(self, off_start, off_end, input_file=None):\n",
    "        self.layer_index = 0\n",
    "        inp = tf.keras.layers.Input(shape=(self.embedding_length,),\n",
    "                                    name=self.layer_name(\"input\"))\n",
    "        x = tf.keras.layers.BatchNormalization(name=self.layer_name(\"batchnorm\"))(inp)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.Dropout(0.5, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.Dropout(0.4, name=self.layer_name(\"dropout\"))(x)\n",
    "        return inp, x\n",
    "\n",
    "    def preprocess_X_train(self, off_start, off_end, input_file=None):\n",
    "        X_train = get_X_text_embed_multilingual(input_file)[off_start:off_end]\n",
    "        X_train = self.std.fit_transform(X_train)\n",
    "        return X_train\n",
    "\n",
    "    def preprocess_X_test(self, off_start, off_end, input_file=None):\n",
    "        X_test = get_X_text_embed_multilingual(input_file)[off_start:off_end]\n",
    "        X_test = self.std.transform(X_test)\n",
    "        return X_test\n",
    "\n",
    "    def fit(self, off_start, off_end, input_file=None):\n",
    "        self.fit_length = off_end - off_start\n",
    "        self.model = self.get_model(off_start, off_end, input_file)\n",
    "\n",
    "        X_train = self.preprocess_X_train(off_start, off_end, input_file)\n",
    "        y_train = self.preprocess_y_train(off_start, off_end, input_file)\n",
    "        \n",
    "        return self.compile_and_train_gen(X_train, y_train, optimizer='adam',\n",
    "                                         epochs=50, patience_stop=8, class_weight=[])\n",
    "        \n",
    "\n",
    "Multilingu().evaluate(NB_ECHANTILLONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e098f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dlology.com/blog/transfer-learning-with-efficientnet/\n",
    "class EffNet(RakutenBaseModel):\n",
    "    \"\"\"\n",
    "    Modèle de transfert learning d'image avec EfficientNet et utilisation\n",
    "    d'un générateur d'images augmentées\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fit_length=None):\n",
    "        super().__init__(self.__class__.__name__, fit_length)\n",
    "\n",
    "    def preprocess_X_train(self, off_start, off_end, input_file=None):\n",
    "        return get_X_image_path(input_file)[off_start:off_end]\n",
    "\n",
    "    def preprocess_X_test(self, off_start, off_end, input_file=None):\n",
    "        return get_X_image_path(input_file)[off_start:off_end]\n",
    "\n",
    "    def get_model_body(self, off_start, off_end, input_file=None):\n",
    "        self.layer_index = 0\n",
    "        self.input_shape = (240, 240, 3) # Dépend du numéro x (EfficientNetBx)\n",
    "        self.preprocessing_function = None # Inclus dans le modèle qui attend des données [..255]\n",
    "        self.basetrainable = 0 ### 20\n",
    "        self.epochs_freezed = 6\n",
    "        self.epochs_unfreezed = 1 #10\n",
    "        self.batch_size = 32\n",
    "        self.validation_split = 0.15\n",
    "        self.basemodel = tf.keras.applications.EfficientNetB1(\n",
    "                                input_shape = self.input_shape,\n",
    "                                include_top = False,\n",
    "                                # drop_connect_rate=0.4,\n",
    "                                weights = 'imagenet')\n",
    "        inp = Input(shape=self.input_shape, name=\"input_\" + self.name)\n",
    "        x = self.basemodel(inp)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = tf.keras.layers.Dense(1024, activation=\"relu\", name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.BatchNormalization(trainable = True,axis=1, name=self.layer_name(\"batchnorm\"))(x)\n",
    "        x = tf.keras.layers.Dropout(0.5, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\", name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.BatchNormalization(trainable = True, axis=1, name=self.layer_name(\"batchnorm\"))(x)\n",
    "        return inp, x\n",
    "\n",
    "    def get_train_dataset(self, X_imagepaths, y_train):\n",
    "        df = pd.DataFrame({\"imgpath\": X_imagepaths, \"class\": y_train})\n",
    "        datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                    validation_split = self.validation_split,\n",
    "                                    preprocessing_function = self.preprocessing_function,\n",
    "                                    rotation_range = 10,\n",
    "                                    width_shift_range = 0.1,\n",
    "                                    height_shift_range = 0.1,\n",
    "                                    zoom_range = 0.1,\n",
    "                                    brightness_range=[0.9, 1.1],\n",
    "                                    horizontal_flip = True\n",
    "                                    )\n",
    "        traingen = datagenerator.flow_from_dataframe(subset = 'training', dataframe=df,\n",
    "                                                     x_col=\"imgpath\", y_col=\"class\",\n",
    "                                                     class_mode=\"sparse\",\n",
    "                                                     target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "                                                     batch_size=self.batch_size)\n",
    "        valgen = datagenerator.flow_from_dataframe(subset='validation',\n",
    "                                                   dataframe=df, x_col=\"imgpath\", y_col=\"class\",\n",
    "                                                   class_mode=\"sparse\",\n",
    "                                                   target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "                                                   batch_size=self.batch_size)\n",
    "        return traingen, valgen\n",
    "\n",
    "    def get_test_dataset(self, X_imagepaths):\n",
    "        df = pd.DataFrame({\"imgpath\": X_imagepaths,\n",
    "                           \"class\": [0 for _ in range(len(X_imagepaths))]})\n",
    "        testgen = tf.keras.preprocessing.image.ImageDataGenerator( \n",
    "                        preprocessing_function = self.preprocessing_function).flow_from_dataframe(\n",
    "                        dataframe=df, x_col=\"imgpath\", y_col=\"class\",\n",
    "                        class_mode=None, target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "                        batch_size=self.batch_size, # ?\n",
    "                        shuffle = False)\n",
    "        return testgen\n",
    "    \n",
    "    def compile_and_train(self, traingen, valgen, optimizer, patience, epochs):\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience,\n",
    "                                             restore_best_weights=True, verbose=1),\n",
    "                     tf.keras.callbacks.ModelCheckpoint(filepath=self.fbestweights,\n",
    "                                             save_weights_only=True, save_best_only=True,\n",
    "                                              monitor='val_loss', mode='min')]\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            os.remove(self.fbestweights)\n",
    "        history = self.model.fit(traingen, epochs=epochs,\n",
    "                                      steps_per_epoch = traingen.n//traingen.batch_size,\n",
    "                                      validation_data = valgen,\n",
    "                                      validation_steps = valgen.n//valgen.batch_size,\n",
    "                                      callbacks=callbacks)\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            self.model.load_weights(self.fbestweights)\n",
    "        plot_history(f\"{self.name}\", history)\n",
    "\n",
    "    def fit(self, off_start, off_end, input_file=\"X_train_update.csv\"):\n",
    "        self.fit_length = off_end - off_start\n",
    "        self.prt(f\"Entrainement avec {self.fit_length} échantillons\")\n",
    "\n",
    "        self.model = self.get_model(off_start, off_end, input_file)\n",
    "\n",
    "        self.prt(f\"Création des générateurs d'images\")\n",
    "        X_imgpaths = get_X_image_path(input_file)[off_start:off_end]\n",
    "        y_train = get_y()[off_start:off_end]\n",
    "        traingen, valgen = self.get_train_dataset(X_imgpaths, y_train)\n",
    "        self.fit_labels = dict((v,k) for k,v in (traingen.class_indices).items())\n",
    "        \n",
    "        self.prt(f\"Congélation des layers du modèle de base et entrainement\")\n",
    "        for layer in self.basemodel.layers:\n",
    "            layer.trainable = False\n",
    "        history = self.compile_and_train(traingen, valgen,\n",
    "                                         optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                                         patience=1,\n",
    "                                         epochs=self.epochs_freezed)\n",
    "        if self.basetrainable > 0:\n",
    "            self.prt(f\"Décongélation des {self.basetrainable} derniers layers et entrainement\")\n",
    "            for layer in self.basemodel.layers[-self.basetrainable:]:\n",
    "                if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                    layer.trainable = True\n",
    "            self.compile_and_train(traingen, valgen,\n",
    "                                   optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
    "                                   patience=2,\n",
    "                                   epochs=self.epochs_unfreezed)\n",
    "        self.prt(\"Entrainement terminé\")\n",
    "        return history\n",
    "\n",
    "    def predict(self, off_start, off_end, input_file=\"X_train_update.csv\"):\n",
    "        length = off_end - off_start\n",
    "        self.prt(f\"Prédiction de {length} échantillons\")\n",
    "        X_imgpaths = get_X_image_path(input_file)[off_start:off_end]\n",
    "        testgen = self.get_test_dataset(X_imgpaths)\n",
    "        \n",
    "        softmaxout = self.model.predict(testgen, verbose = 1)\n",
    "        assert len(self.fit_labels) == NB_CLASSES\n",
    "        y_pred = [self.fit_labels[i] for i in np.argmax(softmaxout, axis=1)]\n",
    "        self.prt(\"Prédiction terminée\")\n",
    "        return y_pred\n",
    "\n",
    "EffNet().evaluate(NB_ECHANTILLONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a1645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ [22h05mn] CatModel: Entrainement avec 16000 échantillons\n",
      "++ [22h05mn] CatModel: Chargements des modèles de base\n",
      "Chargement de l'objet (modele_rakuten_out\\TextOneHot_16000_object.pkl)\n",
      "Chargement du modèle (modele_rakuten_out\\TextOneHot_16000_model.hdf5)\n",
      "Chargement de l'objet (modele_rakuten_out\\Multilingu_16000_object.pkl)\n",
      "Chargement du modèle (modele_rakuten_out\\Multilingu_16000_model.hdf5)\n",
      "Chargement de l'objet (modele_rakuten_out\\EffNet_16000_object.pkl)\n",
      "Chargement du modèle (modele_rakuten_out\\EffNet_16000_model.hdf5)\n",
      "++ [22h05mn] CatModel: Preprocessing des données d'entrainement\n",
      "++ [22h06mn] CatModel: Instantiation des générateurs d'entrainement\n",
      "Nettoyage de modele_rakuten_out\\CatDataset_0_13600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                              | 11/425 [00:00<00:03, 104.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de 425 fichiers de batch dans modele_rakuten_out\\CatDataset_0_13600\n",
      "* modèle TextOneHot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 425/425 [00:04<00:00, 94.25it/s]\n",
      " 37%|████████████████████████████▋                                                 | 156/425 [00:00<00:00, 1548.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* modèle Multilingu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 425/425 [00:00<00:00, 615.63it/s]\n",
      "  0%|▏                                                                                 | 1/425 [00:00<01:15,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* modèle EffNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 425/425 [01:11<00:00,  5.92it/s]\n",
      " 15%|███████████▉                                                                     | 11/75 [00:00<00:00, 106.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage de modele_rakuten_out\\CatDataset_13600_2400\n",
      "Création de 75 fichiers de batch dans modele_rakuten_out\\CatDataset_13600_2400\n",
      "* modèle TextOneHot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 108.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 1212.87it/s]\n",
      "  0%|                                                                                           | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* modèle Multilingu\n",
      "* modèle EffNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:12<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ [22h07mn] CatModel: Création du modèle\n",
      "  * Layers non entrainables:\n",
      "      - input_EffNet\n",
      "      - efficientnetb1\n",
      "      - input_1_TextOneHot\n",
      "      - input_1_Multilingu\n",
      "      - global_average_pooling2d_15\n",
      "      - dense_2_TextOneHot\n",
      "      - batchnorm_2_Multilingu\n",
      "      - dense_1_EffNet\n",
      "      - dropout_3_TextOneHot\n",
      "      - dense_3_Multilingu\n",
      "      - batchnorm_2_EffNet\n",
      "      - dense_4_TextOneHot\n",
      "      - dropout_4_Multilingu\n",
      "      - dropout_3_EffNet\n",
      "      - dropout_5_TextOneHot\n",
      "      - dense_5_Multilingu\n",
      "      - dense_4_EffNet\n",
      "      - batchnorm_6_TextOneHot\n",
      "      - dropout_6_Multilingu\n",
      "      - batchnorm_5_EffNet\n",
      "      - concatenate_1_CatModel\n",
      "  * Layers entrainables:\n",
      "      - dense_2_CatModel\n",
      "      - dropout_3_CatModel\n",
      "      - dense_4_CatModel\n",
      "  * Layers initialisés avec les poids des modèles de base:\n",
      "      - dense_2_TextOneHot\n",
      "      - dense_4_TextOneHot\n",
      "      - batchnorm_6_TextOneHot\n",
      "      - batchnorm_2_Multilingu\n",
      "      - dense_3_Multilingu\n",
      "      - dense_5_Multilingu\n",
      "      - efficientnetb1\n",
      "      - dense_1_EffNet\n",
      "      - batchnorm_2_EffNet\n",
      "      - dense_4_EffNet\n",
      "      - batchnorm_5_EffNet\n",
      "++ [22h07mn] CatModel: fit(): Début\n",
      "Epoch 1/12\n",
      "251/425 [================>.............] - ETA: 5:46 - loss: 2.0971 - accuracy: 0.4621"
     ]
    }
   ],
   "source": [
    "class CatDataset(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Générateur de donnéees utilisé par un modèle à entrées multitles.\n",
    "    Les données préprocessées Xs sont stockéees dans des fichiers qui\n",
    "    seront lus à la demande (__getitem__). Chaque fichier contient les\n",
    "    données d'un batch (pour tous les modèles de base).\n",
    "    \"\"\"\n",
    "    def batch_filepath(self, index):\n",
    "        return os.path.join(self.dir, f\"batch_{index}.npy\")\n",
    "\n",
    "    def __init__(self, batch_size, objs,\n",
    "                 off_start, off_end, Xs, y=None):\n",
    "        length = off_end - off_start\n",
    "        self.batch_size = batch_size\n",
    "        if y is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.y = y[off_start:off_end] # On garde que ce qui sert\n",
    "        self.input_number = len(Xs)\n",
    "        self.batch_number = int(length / batch_size)\n",
    "        self.dir = os.path.join(OUTDIR, f\"{self.__class__.__name__}_{off_start}_{length}\")\n",
    "        if not os.path.isdir(self.dir):\n",
    "            os.makedirs(self.dir)\n",
    "        else:\n",
    "            print(f\"Nettoyage de {self.dir}\")\n",
    "            for f in os.listdir(self.dir):\n",
    "                os.remove(os.path.join(self.dir, f))\n",
    "        print(f\"Création de {self.batch_number} fichiers de batch dans {self.dir}\")\n",
    "        for i, X in enumerate(Xs):\n",
    "            print(f\"* modèle {objs[i].name}\")\n",
    "            X = X[off_start:off_end]\n",
    "            isfile = type(X[0]) == str and os.path.isfile(X[0])\n",
    "            for index in tqdm.tqdm(range(self.batch_number)):\n",
    "                if not isfile:\n",
    "                    X_batch = X[index*batch_size:(index+1)*batch_size, ...]\n",
    "                else:\n",
    "                    # Ce sont des fichiers images\n",
    "                    X_batch = X[index*batch_size:(index+1)*batch_size]\n",
    "                    imgs = list()\n",
    "                    for f in X_batch:\n",
    "                        img = cv2.imread(f)\n",
    "                        img = cv2.resize(img, (240, 240), interpolation=cv2.INTER_CUBIC)\n",
    "                        imgs.append(img)\n",
    "                    X_batch = np.array(imgs)\n",
    "                batchfile = self.batch_filepath(index)\n",
    "                with open(batchfile, 'a+b') as f:\n",
    "                    np.save(f, X_batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Retourne le nombre de batchs\n",
    "        return self.batch_number\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retourne 1 batch, de forme\n",
    "        # X=[(batchsize, X1), (batchsize, X2), ...], y= (batch_size,)\n",
    "        batchfile = self.batch_filepath(index)\n",
    "        X = list() # Liste des batches pour chacun des modèles\n",
    "        with open(batchfile, 'rb') as f:\n",
    "            for _ in range(self.input_number):\n",
    "                objbatch = np.load(f)\n",
    "                X.append(objbatch)\n",
    "        if self.y is None:\n",
    "            return X\n",
    "        else:\n",
    "            y = self.y[index*self.batch_size : (index+1)*self.batch_size]\n",
    "            return X, y\n",
    "\n",
    "    #def on_epoch_end(self):\n",
    "        # option method to run some logic at the end of each epoch: e.g. reshuffling\n",
    "\n",
    "\n",
    "class CatModel(RakutenBaseModel):\n",
    "    \"\"\"\n",
    "    Modèle qui concatene plusieurs modèles de base\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_length=None):\n",
    "        super().__init__(self.__class__.__name__, fit_length)\n",
    "\n",
    "    def create_train_generators(self, off_start, off_end, input_file=None):\n",
    "        self.prt(f\"Preprocessing des données d'entrainement\")\n",
    "        X_train = []\n",
    "        for obj in self.objs:\n",
    "            X_train.append(obj.preprocess_X_train(off_start, off_end, input_file))\n",
    "        y_train = self.preprocess_y_train(off_start, off_end, input_file)\n",
    "        self.prt(f\"Instantiation des générateurs d'entrainement\")\n",
    "        off_val = off_end - int((off_end - off_start) * self.validation_split)\n",
    "        traingen = CatDataset(self.batch_size, self.objs,\n",
    "                              off_start, off_val, X_train, y_train)\n",
    "        valgen = CatDataset(self.batch_size, self.objs,\n",
    "                            off_val, off_end, X_train, y_train)\n",
    "        return traingen, valgen\n",
    "\n",
    "    def create_test_generator(self, off_start, off_end, input_file=None):\n",
    "        self.prt(f\"Preprocessing des données de test\")\n",
    "        X_test = []\n",
    "        for obj in self.objs:\n",
    "            X_test.append(obj.preprocess_X_test(off_start, off_end, input_file))\n",
    "        self.prt(f\"Instantiation du générateur de test\")\n",
    "        testgen = CatDataset(1, self.objs, off_val, off_end, X_test)\n",
    "        return testgen\n",
    "        \n",
    "    def fit(self, off_start, off_end, input_file=None):\n",
    "        self.fit_length = off_end - off_start\n",
    "        self.prt(f\"Entrainement avec {self.fit_length} échantillons\")\n",
    "\n",
    "        self.prt(f\"Chargements des modèles de base\")\n",
    "        # Liste des modèles de base et instanciation, en leur passant la\n",
    "        # taille (fit_length) ce qui revient simplement à lire leurs\n",
    "        # sauvegardes en fichier (il faut que les modèles aient don été\n",
    "        # déjà exécutés et les fichiers créés)\n",
    "        self.objs = [\n",
    "#             TextEmbed(self.fit_length),\n",
    "             TextOneHot(self.fit_length),\n",
    "             Multilingu(self.fit_length),\n",
    "             EffNet(self.fit_length)\n",
    "        ]\n",
    "\n",
    "        # Générateurs qui alimenteront fit() avec les données train & val\n",
    "        traingen, valgen = self.create_train_generators(off_start, off_end, input_file)\n",
    "        \n",
    "        self.prt(f\"Création du modèle\")\n",
    "        # Récupèration des inputs et des outputs de chaque modèle de base\n",
    "        # pour construire le modèle concatenate\n",
    "        inputlayers, outputlayers = [], []\n",
    "        for obj in self.objs:\n",
    "            inp, outp = obj.get_model_body(off_start, off_end, input_file)\n",
    "            inputlayers.append(inp)\n",
    "            outputlayers.append(outp)\n",
    "        \n",
    "        self.layer_index = 0 # compteur utilisé dans get_model_body()\n",
    "        concatname = self.layer_name(\"concatenate\")\n",
    "        x = concatenate(outputlayers, axis=-1, name = concatname)\n",
    "        x = Dense(64, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "        x = Dropout(0.3, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = Dense(NB_CLASSES, activation='softmax', name=self.layer_name(\"dense\"))(x)\n",
    "        self.model = Model(inputlayers, x, name=self.name)\n",
    "\n",
    "        #print(self.model.summary())\n",
    "\n",
    "        print(f\"  * Layers non entrainables:\")\n",
    "        for layer in self.model.layers[:-3]:\n",
    "            layer.trainable = False\n",
    "            print(f\"      - {layer.name}\")\n",
    "        print(f\"  * Layers entrainables:\")\n",
    "        for layer in self.model.layers:\n",
    "            if layer.trainable:\n",
    "                print(f\"      - {layer.name}\")\n",
    "\n",
    "        self.model.compile(optimizer= tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics = ['accuracy'])\n",
    "\n",
    "        print(f\"  * Layers initialisés avec les poids des modèles de base:\")\n",
    "        # Copie des poids des layers de bases vers leurs clones (ayant le\n",
    "        # même nom) du modèle concatenate\n",
    "        for obj in self.objs:\n",
    "            for oldlayer in obj.model.layers:\n",
    "                weights = oldlayer.get_weights()\n",
    "                if len(weights) > 0:\n",
    "                    for newlayer in self.model.layers:\n",
    "                        if newlayer.name == oldlayer.name:\n",
    "                            print(f\"      - {newlayer.name}\")\n",
    "                            newlayer.set_weights(weights)\n",
    "\n",
    "        self.prt(\"fit(): Début\")    \n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,\n",
    "                                             restore_best_weights=True, verbose=1),\n",
    "                     tf.keras.callbacks.ModelCheckpoint(filepath=self.fbestweights,\n",
    "                                             save_weights_only=True, save_best_only=True,\n",
    "                                              monitor='val_loss', mode='min'),\n",
    "                     tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.1,\n",
    "                                                          verbose=1)]\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            os.remove(self.fbestweights)\n",
    "        history = self.model.fit(traingen,\n",
    "                                 epochs=12,\n",
    "                                 validation_data = valgen,\n",
    "                                 callbacks=callbacks)\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            self.model.load_weights(self.fbestweights)\n",
    "        self.prt(\"fit(): Fin\")\n",
    "        plot_history(f\"{self.name}\", history)\n",
    "        return history\n",
    "\n",
    "    def predict(self, off_start, off_end, input_file=None):\n",
    "        length = off_end - off_start\n",
    "        self.prt(f\"Prédiction pour {length} échantillons\")\n",
    "        testgen = self.create_test_generator(off_start, off_end, input_file)\n",
    "         \n",
    "        self.prt(\"predict(): Début\")\n",
    "        softmaxout = self.model.predict(testgen, verbose = 1)\n",
    "        y_pred = [self.fit_labels[i] for i in np.argmax(softmaxout, axis=1)]\n",
    "        self.prt(\"predict(): Fin\\n\")\n",
    "        return y_pred\n",
    "                                  \n",
    "CatModel().evaluate(NB_ECHANTILLONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.utils.plot_model(cat.model, \"to_file=dot_img_file,\n",
    "                          show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
