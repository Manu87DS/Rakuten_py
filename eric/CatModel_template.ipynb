{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52063eb8",
   "metadata": {},
   "source": [
    "Template de notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a58caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten_common import *\n",
    "# NB_ECHANTILLONS=20000 # Par défaut tous les échantillons sont utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457cceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilingu(RakutenBaseModel):\n",
    "    \"\"\"\n",
    "    Modèle de plongement (embedding) dans un modèle de texte préentrainé\n",
    "    (Multilingual-large)\n",
    "    \"\"\"\n",
    "    def __init__(self, nb=None):\n",
    "        super().__init__(self.__class__.__name__, nb)\n",
    "        self.embedding_length = 512 # Défini par le modèle Multilingual/large\n",
    "        self.std = StandardScaler()\n",
    "\n",
    "    def get_model_body(self):\n",
    "        self.layer_index = 0\n",
    "        inp = tf.keras.layers.Input(shape=(self.embedding_length,),\n",
    "                                    name=self.layer_name(\"input\"))\n",
    "        x = tf.keras.layers.BatchNormalization(name=self.layer_name(\"batchnorm\"))(inp)\n",
    "        x = tf.keras.layers.Dense(200, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.Dropout(0.5, name=self.layer_name(\"dropout\"))(x)\n",
    "#        x = tf.keras.layers.Dense(100, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "#        x = tf.keras.layers.Dropout(0.4, name=self.layer_name(\"dropout\"))(x)\n",
    "        return inp, x\n",
    "\n",
    "    def preprocess_X_train(self, off_start, off_end, input_file=None):\n",
    "        X_train = get_X_text_embed_multilingual(input_file)[off_start:off_end]\n",
    "        X_train = self.std.fit_transform(X_train)\n",
    "        return X_train\n",
    "\n",
    "    def preprocess_X_test(self, off_start, off_end, input_file=None):\n",
    "        X_test = get_X_text_embed_multilingual(input_file)[off_start:off_end]\n",
    "        X_test = self.std.transform(X_test)\n",
    "        return X_test\n",
    "\n",
    "    def fit(self, off_start, off_val, off_end, input_file=None):\n",
    "\n",
    "        X_train = self.preprocess_X_train(off_start, off_val, input_file)\n",
    "        y_train = self.preprocess_y_train(off_start, off_val, input_file)\n",
    "\n",
    "        X_val = self.preprocess_X_test(off_val, off_end, input_file)\n",
    "        y_val = self.preprocess_y_train(off_val, off_end, input_file)\n",
    "\n",
    "        self.model = self.get_model()\n",
    "        \n",
    "        return self.compile_and_train_gen(X_train, y_train, X_val, y_val, optimizer='adam',\n",
    "                                         epochs=50, patience_stop=8, patience_lr=3, class_weight=[])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9256673",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multilingu().evaluate(NB_ECHANTILLONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPDataset(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Générateur qui extrait les données (de type tableau numpy)\n",
    "    à partir d'un path de fichier\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size,\n",
    "                 X, y=None, shuffle=False, random_state=1968):\n",
    "        nb = len(X)\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_number = int(nb / batch_size)\n",
    "        self.batch_indexes = range(self.batch_number)\n",
    "        self.prefix = re.match(r'^(.*)_\\d+\\.npy$', X[0]).group(1)\n",
    "        if shuffle:\n",
    "            self.random_state = random_state\n",
    "            self.batch_indexes = sklearn.utils.shuffle(self.batch_indexes,\n",
    "                                                       random_state=random_state)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Retourne le nombre de batchs \"\"\"\n",
    "        return self.batch_number\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.batch_indexes[index]\n",
    "        X = []\n",
    "        for i in range(index*self.batch_size, (index+1)*self.batch_size):\n",
    "            f = f\"{self.prefix}_{i}.npy\"\n",
    "            with open(f, 'rb') as fd:\n",
    "                x = np.load(fd)\n",
    "                X.append(x)\n",
    "        X = np.array(X)\n",
    "        if self.y is None:\n",
    "            return X\n",
    "        else:\n",
    "            y = self.y[index*self.batch_size : (index+1)*self.batch_size]\n",
    "            return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Changements effectués à chaque fin d'époque \"\"\"\n",
    "        if self.shuffle:\n",
    "            self.batch_indexes = sklearn.utils.shuffle(self.batch_indexes,\n",
    "                                                       random_state=self.random_state)\n",
    "        \n",
    "\n",
    "class TextOneHot(RakutenBaseModel):\n",
    "    \"\"\"\n",
    "    Modèle NN exécuté après un préprocessing des données effectué\n",
    "    avec un pipe sklearn <TfidfVectorizer | SelectFromModel(LinearSVC)>\n",
    "    \"\"\"\n",
    "    def __init__(self, nb=None):\n",
    "        super().__init__(self.__class__.__name__, nb)\n",
    "        self.maxfeatures = 200000 # Nombre de features selectionnées par SVC\n",
    "        self.select = make_pipeline(\n",
    "            TfidfVectorizer(analyzer='word',\n",
    "                  #strip_accents='ascii',\n",
    "                  #stop_words=french_stop_words, \n",
    "                  #    tokenizer=tokenize_spacy, # les données sont déjà tokenizées\n",
    "                  preprocessor=' '.join,\n",
    "                  lowercase=False,\n",
    "                  stop_words=None,\n",
    "                  max_df=0.8,\n",
    "                  min_df=2,\n",
    "                  ngram_range=(1,2),\n",
    "                  use_idf=True,\n",
    "                  smooth_idf=True,\n",
    "                  sublinear_tf=False,\n",
    "                  binary=True,\n",
    " #                 max_features=30000,  moins rentable que celui de SelectFromModel \n",
    "                  ),\n",
    "                  SelectFromModel(LinearSVC(penalty=\"l2\", dual=True, C=0.8,\n",
    "                                            tol=1e-5, max_iter=4000),\n",
    "                                  max_features=self.maxfeatures))\n",
    "\n",
    "    def __flush_data_to_file(self, data, tag, off_start, off_end, input_file=None):\n",
    "        filelist = []\n",
    "        for i in range(data.shape[0]):\n",
    "            f = os.path.join(OUTDIR, f\"{self.name}_{tag}_{off_start}_{off_end}_{i}.npy\")\n",
    "            with open(f, 'wb') as fd:\n",
    "                datum = data[i].toarray()[0]\n",
    "                #print(type(datum), datum.shape)\n",
    "                np.save(fd, datum)\n",
    "            filelist.append(f)\n",
    "        return filelist\n",
    "    \n",
    "    def data_from_file(self, filepath):\n",
    "        \"\"\"\n",
    "        Méthode pour lire une donnée preprocessée à partir d'un fichier\n",
    "        et la retourner\n",
    "        \"\"\"\n",
    "        with open(f, 'rb') as fd:\n",
    "            datum = np.load(fd)\n",
    "        return datum\n",
    "\n",
    "    def get_model_body(self):\n",
    "        self.layer_index = 0\n",
    "        inp = tf.keras.layers.Input(shape=self.input_shape, name=self.layer_name(\"input\"))\n",
    "        x = inp\n",
    "#        x = tf.keras.layers.Dense(90, activation='relu', name=self.layer_name(\"dense\"))(inp)\n",
    "#        x = tf.keras.layers.Dropout(0.5, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=self.layer_name(\"batchnorm\"))(x)\n",
    "        return inp, x\n",
    "\n",
    "    def preprocess_X_train(self, off_start, off_end, input_file=None):\n",
    "        y_train = get_y()[off_start:off_end]\n",
    "#        X_train = get_X_text(input_file)[off_start:off_end]\n",
    "        X_train = get_X_text_spacy_lemma(input_file)[off_start:off_end]\n",
    "        X_train = self.select.fit_transform(X_train, y_train)\n",
    "        self.input_shape = X_train.shape[1:]\n",
    "        self.prt(f\"Nombre de mots retenus = {self.input_shape[0]}\")\n",
    "        return self.__flush_data_to_file(X_train, \"train\", off_start, off_end)\n",
    "\n",
    "    def preprocess_X_test(self, off_start, off_end, input_file=None):\n",
    "#        X_test = get_X_text(input_file)[off_start:off_end]\n",
    "        X_test = get_X_text_spacy_lemma(input_file)[off_start:off_end]\n",
    "        X_test = self.select.transform(X_test)\n",
    "        return self.__flush_data_to_file(X_test, \"test\", off_start, off_end)\n",
    "\n",
    "    def fit(self, off_start, off_val, off_end, input_file=None):\n",
    "\n",
    "        X_train = self.preprocess_X_train(off_start, off_val, input_file)\n",
    "        y_train = self.preprocess_y_train(off_start, off_val, input_file)\n",
    "        trainds = NPDataset(self.batch_size, X_train, y_train, shuffle=True)\n",
    "\n",
    "        X_val = self.preprocess_X_test(off_val, off_end, input_file)\n",
    "        y_val = self.preprocess_y_train(off_val, off_end, input_file)\n",
    "        valds = NPDataset(self.batch_size, X_val, y_val, shuffle=True)\n",
    "\n",
    "        self.model = self.get_model()\n",
    "\n",
    "        return self.compile_and_train_dataset(trainds, valds,\n",
    "                                              optimizer=tf.keras.optimizers.Adam(lr=0.0002),\n",
    "                                              epochs=50,\n",
    "                                              patience_stop=7, patience_lr=4)\n",
    "\n",
    "    def predict(self, off_start, off_end, input_file=None):\n",
    "        X_test = self.preprocess_X_test(off_start, off_end, input_file)\n",
    "        testds = NPDataset(1, X_test, shuffle=False)\n",
    "        return self.model_predict(testds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4250a15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TextOneHot().evaluate(NB_ECHANTILLONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e15730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNet(RakutenBaseModel):                                                 #! Nom de la classe\n",
    "    \"\"\"\n",
    "    Modèle de transfert learning d'images avec EfficientNet et l'utilisation\n",
    "    d'un générateur d'images augmentées.\n",
    "    https://www.dlology.com/blog/transfer-learning-with-efficientnet/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb=None):\n",
    "        super().__init__(self.__class__.__name__, nb)\n",
    "        self.input_shape = (240, 240, 3) # Dépend du numéro x (EfficientNetBx)  #! Shape\n",
    "        self.preprocessing_function = None # Inclus dans le modèle              #! Preprocessor\n",
    "        self.basetrainable = 16\n",
    "        self.epochs_freezed = 10\n",
    "        self.epochs_unfreezed = 10\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def preprocess_X_train(self, off_start, off_end, input_file=None):\n",
    "        return get_X_image_path(input_file)[off_start:off_end]\n",
    "\n",
    "    def preprocess_X_test(self, off_start, off_end, input_file=None):\n",
    "        return get_X_image_path(input_file)[off_start:off_end]\n",
    "    \n",
    "    def data_from_file(self, filepath):\n",
    "        \"\"\"\n",
    "        Méthode pour lire une image à partir d'un fichier et la retourner\n",
    "        au bon format (adapté aux entrées du modèle modèle)\n",
    "        \"\"\"\n",
    "        img = cv2.imread(filepath)\n",
    "        img = cv2.resize(img, (240, 240), interpolation=cv2.INTER_CUBIC)        #! Shape\n",
    "        return img\n",
    "\n",
    "    def get_model_body(self):\n",
    "        self.layer_index = 0\n",
    "        self.basemodel = tf.keras.applications.EfficientNetB1(                  #!\n",
    "                                input_shape = self.input_shape,                 #!\n",
    "                                include_top = False,                            #! Modèle\n",
    "                                # drop_connect_rate=0.4,                        #!\n",
    "                                weights = 'imagenet')                           #!\n",
    "        inp = Input(shape=self.input_shape, name=\"input_\" + self.name)\n",
    "        x = self.basemodel(inp)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = tf.keras.layers.Dense(1024, activation=\"relu\", name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.BatchNormalization(trainable = True,axis=1, name=self.layer_name(\"batchnorm\"))(x)\n",
    "        x = tf.keras.layers.Dropout(0.5, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\", name=self.layer_name(\"dense\"))(x)\n",
    "        x = tf.keras.layers.BatchNormalization(trainable = True, axis=1, name=self.layer_name(\"batchnorm\"))(x)\n",
    "        return inp, x\n",
    "\n",
    "    def __get_train_dataset(self, X_imagepaths, y_train):\n",
    "        df = pd.DataFrame({\"imgpath\": X_imagepaths, \"class\": y_train})\n",
    "        datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                    validation_split = VALIDATION_SPLIT,\n",
    "                                    preprocessing_function = self.preprocessing_function,\n",
    "                                    rotation_range = 10,\n",
    "                                    width_shift_range = 0.1,\n",
    "                                    height_shift_range = 0.1,\n",
    "                                    zoom_range = 0.1,\n",
    "                                    brightness_range=[0.9, 1.1],\n",
    "                                    horizontal_flip = True\n",
    "                                    )\n",
    "        traingen = datagenerator.flow_from_dataframe(subset = 'training', dataframe=df,\n",
    "                                                     x_col=\"imgpath\", y_col=\"class\",\n",
    "                                                     class_mode=\"sparse\",\n",
    "                                                     target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "                                                     batch_size=self.batch_size)\n",
    "        valgen = datagenerator.flow_from_dataframe(subset='validation',\n",
    "                                                   dataframe=df, x_col=\"imgpath\", y_col=\"class\",\n",
    "                                                   class_mode=\"sparse\",\n",
    "                                                   target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "                                                   batch_size=self.batch_size)\n",
    "        return traingen, valgen\n",
    "\n",
    "    def __get_test_dataset(self, X_imagepaths):\n",
    "        df = pd.DataFrame({\"imgpath\": X_imagepaths,\n",
    "                           \"class\": [0 for _ in range(len(X_imagepaths))]})\n",
    "        testgen = tf.keras.preprocessing.image.ImageDataGenerator( \n",
    "                        preprocessing_function = self.preprocessing_function).flow_from_dataframe(\n",
    "                        dataframe=df, x_col=\"imgpath\", y_col=\"class\",\n",
    "                        class_mode=None, target_size=(self.input_shape[0], self.input_shape[1]),\n",
    "                        batch_size=1,\n",
    "                        shuffle = False)\n",
    "        return testgen\n",
    "\n",
    "    def fit(self, off_start, off_val, off_end, input_file=None):\n",
    "\n",
    "        self.prt(f\"Création des générateurs d'images\")\n",
    "        X_imgpaths = get_X_image_path(input_file)[off_start:off_end]\n",
    "        y_train = get_y()[off_start:off_end]\n",
    "        traingen, valgen = self.__get_train_dataset(X_imgpaths, y_train)\n",
    "        self.fit_labels = dict((v,k) for k,v in (traingen.class_indices).items())\n",
    "\n",
    "        self.model = self.get_model()\n",
    "        \n",
    "        self.prt(f\"Congélation des layers du modèle de base et entrainement\")\n",
    "        for layer in self.basemodel.layers:\n",
    "            layer.trainable = False\n",
    "        history = self.compile_and_train_dataset(traingen, valgen,\n",
    "                                         optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                                         epochs=self.epochs_freezed)\n",
    "        if self.basetrainable > 0:\n",
    "            self.prt(f\"Décongélation des {self.basetrainable} derniers layers et entrainement\")\n",
    "            for layer in self.basemodel.layers[-self.basetrainable:]:\n",
    "                if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                    layer.trainable = True\n",
    "            self.compile_and_train_dataset(traingen, valgen,\n",
    "                                   optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
    "                                   epochs=self.epochs_unfreezed)\n",
    "        self.prt(\"Entrainement terminé\")\n",
    "        return history\n",
    "\n",
    "    def predict(self, off_start, off_end, input_file=None):\n",
    "        X_test = self.preprocess_X_test(off_start, off_end, input_file)\n",
    "        testgen = self.__get_test_dataset(X_test)\n",
    "        return self.model_predict(testgen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cb1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "EffNet().evaluate(NB_ECHANTILLONS)                                   #! Execution pour la classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a144b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatModel(RakutenCatModel):\n",
    "    \"\"\"\n",
    "    Modèle qui concatene plusieurs modèles de base\n",
    "    \"\"\"\n",
    "    def __init__(self, nb=None):\n",
    "        super().__init__(self.__class__.__name__, nb)\n",
    "        \n",
    "    def fit(self, off_start, off_val, off_end, input_file=None):\n",
    "        \"\"\"\n",
    "        Entrainement du modèle sur l'intervalle des données spécifié par\n",
    "        off_start et off_end (offsets dans les fichiers de data).\n",
    "        \"\"\"\n",
    "\n",
    "        self.prt(f'Chargements des modèles référencés \"{self.nb}\"')\n",
    "        self.objs = [\n",
    "             TextOneHot(self.nb),\n",
    "             Multilingu(self.nb),\n",
    "             EffNet(self.nb)\n",
    "        ]\n",
    "\n",
    "        # Générateurs qui alimenteront fit() avec les données train & val\n",
    "        traingen, valgen = self.create_train_generators(off_start, off_val, off_end, input_file)\n",
    "        \n",
    "        self.prt(f\"Création du modèle\")\n",
    "        # Récupèration des inputs et des outputs de chaque modèle de base\n",
    "        # pour construire le modèle concatenate\n",
    "        inputlayers, outputlayers = [], []\n",
    "        for obj in self.objs:\n",
    "            inp, outp = obj.get_model_body()\n",
    "            inputlayers.append(inp)\n",
    "            outputlayers.append(outp)\n",
    "        \n",
    "        self.layer_index = 0 # compteur utilisé dans get_model_body()\n",
    "        concatname = self.layer_name(\"concatenate\")\n",
    "        x = concatenate(outputlayers, axis=-1, name = concatname)\n",
    "        x = Dense(64, activation='relu', name=self.layer_name(\"dense\"))(x)\n",
    "        x = Dropout(0.3, name=self.layer_name(\"dropout\"))(x)\n",
    "        x = Dense(NB_CLASSES, activation='softmax', name=self.layer_name(\"dense\"))(x)\n",
    "        self.model = Model(inputlayers, x, name=self.name)\n",
    "\n",
    "        #print(self.model.summary())\n",
    "\n",
    "        print(f\"  * Layers non entrainables:\")\n",
    "        for layer in self.model.layers[:-3]:\n",
    "            layer.trainable = False\n",
    "            print(f\"      - {layer.name}\")\n",
    "        print(f\"  * Layers entrainables:\")\n",
    "        for layer in self.model.layers:\n",
    "            if layer.trainable:\n",
    "                print(f\"      - {layer.name}\")\n",
    "\n",
    "        self.model.compile(optimizer= tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics = ['accuracy'])\n",
    "\n",
    "        # Initialisation des poids\n",
    "        self.copy_submodels_weights()\n",
    "\n",
    "        self.prt(\"fit(): Début\")    \n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,\n",
    "                                             restore_best_weights=True, verbose=1),\n",
    "                     tf.keras.callbacks.ModelCheckpoint(filepath=self.fbestweights,\n",
    "                                             save_weights_only=True, save_best_only=True,\n",
    "                                              monitor='val_loss', mode='min'),\n",
    "                     tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.1,\n",
    "                                                          verbose=1)]\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            os.remove(self.fbestweights)\n",
    "        history = self.model.fit(traingen,\n",
    "                                 epochs=12,\n",
    "                                 validation_data = valgen,\n",
    "                                 callbacks=callbacks)\n",
    "        if os.path.isfile(self.fbestweights):\n",
    "            self.model.load_weights(self.fbestweights)\n",
    "        self.prt(\"fit(): Fin\")\n",
    "        plot_history(f\"{self.name}\", history)\n",
    "        return history\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f508e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatModel().evaluate(NB_ECHANTILLONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
